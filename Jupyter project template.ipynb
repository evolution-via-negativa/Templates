{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Название проекта\n",
    "\n",
    "**Описание проекта:**\n",
    "\n",
    "**Цель исследования —**\n",
    "\n",
    "**Описание данных:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Содержание проекта:\n",
    "\n",
    " - [Шаг 1. Подключение к базе. Загрузка таблицы sql](#getting-data)\n",
    " - [Шаг 2. Первичное исследование таблиц](#exploratory-data-analysis)\n",
    " - [Шаг 3. Статистический анализ факторов ДТП](#statistical-factor-analysis)\n",
    " - [Шаг 4. Создание модели для оценки водительского риска](#model-creation)\n",
    " - [Шаг 5. Поиск лучшей модели и анализ важности факторов ДТП](#best-model-search)\n",
    " - [Шаг 6. Проверка лучшей модели в работе](#best-model-test)\n",
    " - [Шаг 7. Выводы](#general-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем полезные библиотеки, объявим константы и зададим параметры по умолчанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install phik optuna torchvision spacy~=3.2.6 scikit-learn~=1.5.0\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# database\n",
    "import psycopg2\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# math and optimization\n",
    "import math\n",
    "import scipy.stats as st\n",
    "import phik\n",
    "import optuna\n",
    "from phik.report import plot_correlation_matrix\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# utility\n",
    "import itertools\n",
    "import copy\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "# time series\n",
    "import calendar\n",
    "import datetime\n",
    "from time import time\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupShuffleSplit \n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score, fbeta_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# lightgbm\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor, plot_importance\n",
    "\n",
    "# neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# natural language processing\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# computer vision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, GlobalAveragePooling2D\n",
    "\n",
    "# constant values\n",
    "RANDOM_STATE = 884002\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 24\n",
    "\n",
    "# set default values\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "plt.rc('font', size=SMALL_SIZE)                                    # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)                              # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)                              # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)                              # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)                              # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)                              # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)                            # fontsize of the figure title\n",
    "plt.rc('figure', figsize=(18, 12))                                 # controls figure size\n",
    "sns.set(rc={'figure.figsize':(18, 12)})\n",
    "tqdm.pandas()\n",
    "set_config(display='diagram')\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "#Функция для исследовательского анализа данных\n",
    "def exploratory_data_analysis(table, bad_columns):\n",
    "    good_columns = table.columns.difference(bad_columns)\n",
    "    good_table = table[good_columns]\n",
    "    \n",
    "    # Ознакомимся с набором данных\n",
    "    display(good_table.head())\n",
    "    \n",
    "    # Отобразим информацию для краткого обзора данных\n",
    "    good_table.info()\n",
    "    \n",
    "    # Отобразим таблицу с описательной статистикой столбцов\n",
    "    display(good_table.describe())\n",
    "    \n",
    "    # Отобразим количественные и категориальные переменные соответствующими методами\n",
    "    fig = plt.figure(figsize=(24, 12))\n",
    "    plt.subplots_adjust(wspace=0.25, hspace=0.8)\n",
    "\n",
    "    for i, col in enumerate(good_columns):\n",
    "        ax = fig.add_subplot(4, 3, i + 1)\n",
    "\n",
    "        if np.issubdtype(data[col].dtype, np.number):\n",
    "            good_table[col].plot(kind='hist')\n",
    "        else:\n",
    "            good_table[col].value_counts().plot(kind='bar')\n",
    "\n",
    "        ax.set_title(col)\n",
    "        ax.grid(visible=True)\n",
    "        \n",
    "    # Отобразим таблицу с попарными корреляциями столбцов    \n",
    "    phik_overview = good_table[good_columns].phik_matrix(interval_cols=[''])\n",
    "    plot_correlation_matrix(phik_overview.values, x_labels=phik_overview.columns, y_labels=phik_overview.index, \n",
    "                            vmin=0, vmax=1, color_map='Blues',\n",
    "                            title=r'correlation $\\phi_K$', fontsize_factor=1.5, figsize=(24, 12))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и разведочный анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключимся к базе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_config = {\n",
    "    'user': '',\n",
    "    'password': '',\n",
    "    'host': '',\n",
    "    'port': ,\n",
    "    'dbname': ''\n",
    "} \n",
    " \n",
    "conn = psycopg2.connect(**database_config) \n",
    "cur = conn.cursor() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим соответствует ли количество таблиц условию задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema=''\")\n",
    "table_names = pd.Series(data=[data_tuple[0] for data_tuple in cur.fetchall()], name='table_names')\n",
    "table_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим все ли таблицы содержат данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_not_empty = pd.Series(index=table_names.values, dtype='bool', name='table_not_empty')\n",
    "for table_name in table_names.values:\n",
    "    cur.execute(f\"SELECT EXISTS(SELECT 1 FROM .{table_name})\")\n",
    "    table_not_empty[table_name] = cur.fetchall()[0][0]\n",
    "    \n",
    "table_not_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим все таблицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {}\n",
    "\n",
    "for table_name in table_names:\n",
    "    cur.execute(f\"SELECT * FROM .{table_name}\")\n",
    "    tables[table_name] = pd.DataFrame(data=cur.fetchall(),\n",
    "                                      columns=[desc[0] for desc in cur.description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получим данные безопасным способом при помощи конструкции try-except:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('/datasets/real_estate_data.csv')\n",
    "except:\n",
    "    data = pd.read_csv('https://code.s3.yandex.net/datasets/real_estate_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ознакомимся с набором данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_data_analysis(data, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Закроем соединение с базой данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close() \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Краткий вывод:\n",
    "В данных были обнаружены следующие проблемы (или их отсутствие):\n",
    "- в данных присутствуют/отсутствуют неинформативные столбцы\n",
    "- в данных присутствуют/отсутствуют нарушения правила хорошего стиля в названиях столбцов\n",
    "- в данных присутствуют/отсутствуют несоответствия типов в столбцах ``\n",
    "- в данных присутствуют/отсутствуют пропуски в столбцах\n",
    "- в данных присутствуют/отсутствуют явные и неявные дубликаты\n",
    "- в данных присутствуют/отсутствуют аномальные значения\n",
    "- в данных присутствуют/отсутствуют намёки на мультиколлинеарность\n",
    "- в данных присутствуют/отсутствуют намёки на дисбаланс классов в целевом признаке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исправим нарушения правил хорошего стиля в названиях столбцов:\n",
    "* несколько слов в названии запишем в «змеином_регистре»\n",
    "* все символы сделаем строчными\n",
    "* устраним пробелы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(inplace=True, columns={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исправим несоответствия типов в столбцах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype({})\n",
    "\n",
    "# check\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим новые столбцы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Категоризуем данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удалим неинформативные столбцы, которые не несут ценности для прогноза::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([''], axis=1, inplace=True)\n",
    "\n",
    "# check\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработаем пропущенные значения:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала посчитаем, сколько в таблице пропущенных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем категориальные пропуски"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Различают следующие 3 механизма формирования пропусков:\n",
    "1. **MCAR**. Механизм формирования пропусков, при котором вероятность пропуска для каждой записи набора одинакова. Столбцы, которые имеют такой механизм формирования пропусков:\n",
    "    - ` `\n",
    "2. **MAR**. Механизм формирования пропусков, при котором вероятность пропуска может быть определена на основе другой имеющейся в наборе данных информации, не содержащей пропуски. Столбцы, которые имеют такой механизм формирования пропусков:\n",
    "    - ` `\n",
    "3. **MNAR**. Механизм формирования пропусков, при котором вероятность пропуска могла бы быть описана на основе других атрибутов, но информация по этим атрибутам в наборе данных отсутствует (например, объект недвижимости может иметь подземный этаж или чердак, но такая информация в наборе данных отсутствует). Столбцы, которые имеют такой механизм формирования пропусков:  \n",
    "    - ` `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем количественные пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что в таблице не осталось пропусков. Для этого ещё раз посчитаем пропущенные значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработаем дубликаты:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала обработаем неявные дубликаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем на экран количество полных строк-дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим полные дубликаты из таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем на экран неполные дубликаты по подмножеству стобцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.duplicated(subset=[])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим неполные дубликаты из таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё раз посчитаем явные дубликаты в таблице и убедимся, что полностью от них избавились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработаем аномальные значения:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учтем особенности фильтрации `pandas`, чтобы не потерять записи с пропусками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Краткий вывод:\n",
    "В данных были устранены следующие проблемы:\n",
    "- нарушения правил хорошего стиля в названиях столбцов\n",
    "- несоответствия типов в столбцах\n",
    "- неинформативные признаки, которые не несут ценности для прогноза\n",
    "- пропуски в столбцах ``\n",
    "- явные и неявные дубликаты\n",
    "- аномальные значения в столбцах ``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исследовательский анализ данных:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исследуем баланс классов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проанализируем как целевой признак `` связан со всеми остальными признаками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = data.corr()[''].sort_values(key=abs)\n",
    "abs(corr_data).plot(kind='barh', \n",
    "                    fontsize=14,\n",
    "                    color=(corr_data > 0).map({True: 'g', False: 'r'}),\n",
    "                    edgecolor='black',\n",
    "                    title='',\n",
    "                    grid=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделим исходные данные на обучающую, валидационную и тестовую выборки в соотношении $60\\%$/$20\\%$/$20\\%$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=RANDOM_STATE, stratify=data.is_ultra)\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.25, random_state=RANDOM_STATE, stratify=train_data.is_ultra)\n",
    "\n",
    "# check\n",
    "split = pd.DataFrame(index=['train', 'valid', 'test'],\n",
    "                     columns=['size', 'size_proportion'],\n",
    "                     data=[[dataset.shape[0], round(dataset.shape[0] / data.shape[0] * 100, 2)], \n",
    "                           for dataset in [train_data, valid_data, test_data]])\n",
    "\n",
    "display(split)\n",
    "\n",
    "# Разделим обучающую выборку\n",
    "train_features = train_data.drop([''], axis=1) \n",
    "train_target = train_data['']\n",
    "\n",
    "# Разделим валидационную выборку\n",
    "valid_features = valid_data.drop([''], axis=1)\n",
    "valid_target = valid_data['']\n",
    "\n",
    "# Разделим тестовую выборку\n",
    "test_features = test_data.drop([''], axis=1)\n",
    "test_target = test_data['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Краткий вывод:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим объекты, которые понадобятся нам в дальнейшем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powers_of_two = [2**i for i in range(1, 7)]\n",
    "cv_results = pd.DataFrame(columns=['MAE'])\n",
    "model_params = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим базовый пайплайн для предобработки данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline(steps=[('', )])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Напишем функцию для кросс-валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_cv(trial, objective, n_splits):\n",
    "    fold = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, valid_idx in fold.split(range(len(train_data))):\n",
    "        train_features = train_data.iloc[train_idx].drop([''], axis=1)\n",
    "        valid_features = train_data.iloc[valid_idx].drop([''], axis=1)\n",
    "        train_target = train_data.iloc[train_idx]['']\n",
    "        valid_target = train_data.iloc[valid_idx]['']\n",
    "\n",
    "        dataset = ((train_features, train_target), (valid_features, valid_target))\n",
    "        scores.append(objective(trial, dataset))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим и обучим наивную модель:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При помощи перебора гиперпараметров найдём наилучшую наивную модель решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, dataset):\n",
    "    (train_features, train_target), (valid_features, valid_target) = dataset\n",
    "    \n",
    "    regressor = DummyRegressor()\n",
    "    regressor.strategy = trial.suggest_categorical('strategy', ['mean', 'median'])\n",
    "        \n",
    "    model = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', regressor)])\n",
    "    model.fit(train_features, train_target)\n",
    "    return mean_absolute_error(valid_target, model.predict(valid_features))\n",
    "\n",
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "study.optimize(partial(objective_cv, objective=objective, n_splits=5), n_trials=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим результаты кросс-валидации в таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params['DummyRegressor'] = study.best_trial.params\n",
    "cv_results.loc['DummyRegressor', 'MAE'] = study.best_trial.values[0]\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим базовой нейронную сеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс полносвязной нейронной сети с произвольным числом слоёв и нейронов на каждом слое\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, layers_neurons, dropout_layers, act_functions):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(layers_neurons[i], layers_neurons[i+1]) for i in range(len(layers_neurons) - 1)])\n",
    "        self.dropout_layers = nn.ModuleList(dropout_layers)\n",
    "        self.act_functions = nn.ModuleList(act_functions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            x = self.dropout_layers[i](x)\n",
    "            x = self.act_functions[i](x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "# Класс модели нейронной сети для работы с библиотекой sklearn\n",
    "class NNRegressor(BaseEstimator):\n",
    "    def __init__(self, net, optimizer, num_epochs=1000, batch_size=512, verbose=False, verbose_epochs=100, **parameters):\n",
    "        self.net = net\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        self.verbose_epochs = verbose_epochs\n",
    "        \n",
    "        if 'dropout_probs' in parameters:\n",
    "            for i in range(len(parameters['dropout_probs'])):\n",
    "                self.net.dropout_layers[i].p = parameters['dropout_probs'][i]\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'net' : self.net,\n",
    "            'optimizer': self.optimizer,\n",
    "            'loss': self.loss,\n",
    "            'num_epochs': self.num_epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'verbose': self.verbose,\n",
    "            'verbose_epochs': self.verbose_epochs\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "            \n",
    "        if 'dropout_probs' in parameters:\n",
    "            for i in range(len(parameters['dropout_probs'])):\n",
    "                self.net.dropout_layers[i].p = parameters['dropout_probs'][i]\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def fit(self, X_train, y_train):                               \n",
    "        if isinstance(X_train, (pd.DataFrame, pd.Series)):\n",
    "            X_train = X_train.to_numpy(copy=True)\n",
    "        if isinstance(y_train, (pd.DataFrame, pd.Series)):\n",
    "            y_train = y_train.to_numpy(copy=True)\n",
    "\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        \n",
    "        num_batches = math.ceil(len(X_train) / self.batch_size)\n",
    "        self.net.train()\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            order = np.random.permutation(len(X_train))\n",
    "            \n",
    "            for batch_idx in range(num_batches):\n",
    "                start_index = batch_idx * self.batch_size\n",
    "                self.optimizer.zero_grad()\n",
    "        \n",
    "                batch_indexes = order[start_index:start_index+self.batch_size]\n",
    "                X_batch = X_train[batch_indexes]\n",
    "                y_batch = y_train[batch_indexes]\n",
    "\n",
    "                preds = self.net.forward(X_batch).flatten()\n",
    "                loss_value = self.loss(preds, y_batch)\n",
    "                loss_value.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if self.verbose and epoch % self.verbose_epochs == 0:\n",
    "                print(loss_value)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if isinstance(X_test, (pd.DataFrame, pd.Series)):\n",
    "            X_test = X_test.to_numpy(copy=True)\n",
    "\n",
    "        self.net.eval()\n",
    "        return self.net.forward(torch.FloatTensor(X_test)).detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net([5, 100, 100, 1], [nn.Dropout(0.5), nn.Dropout(0.5), nn.Identity()], [nn.ReLU(), nn.ReLU(), nn.Identity()])\n",
    "nn.init.kaiming_uniform_(net.layers[0].weight, mode='fan_in', nonlinearity='relu')\n",
    "nn.init.kaiming_uniform_(net.layers[1].weight, mode='fan_in', nonlinearity='relu') \n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Проанализируем результаты моделирования и выберем наилучшую модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = study.trials_dataframe()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Краткий вывод:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование и анализ наилучшей модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(columns=['MAE', 'R2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим наилучшую модель на объединении обучающей и валидационной выборок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_data.drop(['temperature_last'], axis=1)\n",
    "test_target = test_data['temperature_last']\n",
    "\n",
    "best_model = LGBMRegressor(**model_params['LGBMRegressor'])\n",
    "best_model.fit(train_data.drop(['temperature_last'], axis=1), train_data['temperature_last'])\n",
    "best_predictions = best_model.predict(test_features) \n",
    "\n",
    "test_results.loc['', 'MAE'] = mean_absolute_error(test_target, best_predictions)\n",
    "test_results.loc['', 'R2'] = r2_score(test_target, best_predictions)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Протестируем наилучшую модель на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_one_valid = forest_model.predict_proba(valid_features)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(valid_target, probabilities_one_valid)\n",
    "\n",
    "plt.figure()\n",
    "# ROC-кривая модели RandomForestClassifier\n",
    "plt.plot(fpr, tpr)\n",
    "\n",
    "# ROC-кривая случайной модели (выглядит как прямая)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim(0, 1.0)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-кривая')\n",
    "plt.show()\n",
    "\n",
    "print('Площадь под ROC-кривой (AUC-ROC) равна:', roc_auc_score(valid_target, probabilities_one_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведём графический анализ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test,\n",
    "                                                         y_pred,\n",
    "                                                         normalize='all'),\n",
    "                                                         display_labels=['at_fault', 'not at_fault']).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrecisionRecallDisplay.from_estimator(pipeline, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проанализируем важность основных факторов, влияющих на... :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_importance(classifier, ignore_zero=False)\n",
    "ax.set_yticklabels(pipeline[:-1].get_feature_names_out());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проанализируем зависимость целевого признака ` ` от ...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(pipeline, test_features, ['vehicle_age'], line_kw={\"color\": \"red\"});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Краткий вывод:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий вывод:"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 60,
    "start_time": "2024-06-26T14:30:03.947Z"
   },
   {
    "duration": 47,
    "start_time": "2024-06-26T14:31:09.288Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
